{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines \n",
    " <br> \n",
    " SVM은 두범주를 분류하기위해 변수공간(feature space)의 __초평면(Hyperplane)__을 찾는 알고리즘이다.<br>\n",
    "하지만 대부분 분류문제의 경우 두 범주를 정확히 나누는 초평면(Hyperplane)을 찾을 수 없는 경우가 많아 다음과 같은 방법으로 문제를 해결한다. <br>\n",
    "1. 분류기준을 부드럽게?(soften) 만들어 어느정도의 오차를 인정한다.\n",
    "2. 변수공간을 더 높은 차원(dimension)으로 확장시켜, Hyperplane으로 분류가 가능하도록 만든다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대표적으로 다음과 같은 3가지 SVM 모델이 있다. <br>\n",
    "1. Maximal margin classfier\n",
    "2. Support vector classifier(soft margin classifier)\n",
    "3. Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperplane\n",
    "Hyperplane이란 p차원 변수공간에서의 (p-1)차원으로 만들어진 __초평면__이다. <br>\n",
    "일반적으로 hyperplane은 다음과 같은 식으로 나타낼 수 있다. <br>\n",
    "### <center>$\\beta_{0} + \\beta_{1}X_{1} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p} = 0$ </center> \n",
    "만약 특정 데이터 $X = (X1, ..., Xp)^{T}$가 위의 식을 만족한다면, X는 hyperplane 에 놓이게 된다. <br>\n",
    "만약 데이터 $X$가 위의 식을 만족하지 않으면, 데이터는 hyperplane기준 한 방향에 놓이게 될 것이고, 놓인 방향에 따라 다른 __부호(sing)__를 가지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img1](img1.JPG)\n",
    "\n",
    "<center>- Blue region : $1 + 2X_{1} +3X_{2} > 0$ </center>\n",
    "<center>- Red region : $1 + 2X_{1} +3X_{2} < 0$ </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperplane $f(X)$는 다음과 같이 vector notation으로 쓸 수 있다.\n",
    "\n",
    "#### <center>$f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p} = \\beta^{T}X$ </center> \n",
    "$\\beta = (\\beta_{1},\\beta_{1},..,\\beta_{p})^{T}$는 __법선 벡터(normal vector)__라 불린다. <br>\n",
    "위의 normal vector는 hyperplane과 __직교(orthogonal)__한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](img2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Margin Classifier\n",
    "__maximal margin hyperplane__은 훈련 데이터의 두 범주사이의 거리를 가장 멀도록 만든 hyperplane이다. <br>\n",
    "즉, 가능한 모든 hyperplane에서 두 범주 사이의 __margin__(거리)를 가장 크게 만드는 hyperplane을 찾는다. \n",
    "#### $maximaize_{\\substack{\\beta_{0},.,\\beta_{p}}} \\; M  \\quad$  subject to \n",
    "#### <center> $\\sum^{p}_{j=1}\\beta_{j}^2 = 1, \\quad y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + ... + \\beta_{p}x_{ip}) \\geq M$ </center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $M$은 margin의 크기로, 가능한 크도록 $\\beta$를 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img3](img3.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위와 같은 classifer를 만들기 위해 총 3개의 점이 사용 되었는데, 이러한 점들을 __support vector__라 부른다.\n",
    "- 일반적으로 p차원 변수공간에서 p+1개의 support vector가 필요하다.\n",
    "- maximal margin classfier는 만약 위와 같이 두 범주를 분류하는 hyperplane이 존재한다면, 결과를 얻을 수 있다.\n",
    "- 하지만, 많은 경우 두 범주를 분류하는 hyperplane은 존재하지 않으며, 이 때 maximal margin classfier는 존재하지 않는다.\n",
    "- 따라서 모든 범주를 분류하도록 margin을 maximaize하는 것이 아니라, __soft margin__ 을 maximaize한다.\n",
    "- 이와 같이 soft margin을 이용하는 분류 모델을 __support vector classifier__라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector classfier\n",
    "모든 데이터들을 완벽하게 두 범주로 분리하지 못하더라도 두 가지 관점에서 바라볼 필요가 있다. \n",
    "- 개별 데이터에 대해 더 강인함(robustness)를 가지는가?\n",
    "- 더 나은 분류 결과를 가지는가?\n",
    "![img4](img4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오른쪽 그림에서 실선보다는 점선의 hyperplnae을 채택하는 것이, 새로 들어온 데이터 분류에 있어 더 좋은 모델이다. <br>\n",
    "즉, 새로운 데이터가 들어왔을 때 더 좋은 분류를 위해, hyperplane을 만드는 과정에서 몇몇 데이터는 잘못 분류하는 것이 오히려 더 가치있는 결과를 나을 수 있다. <br>\n",
    "__support vector classifer(soft margin classifier)__는 몇몇 데이터의 오분류를 허용함으로써 individual observation에 대해 강인한 결과를 낳는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $maximaize_{\\substack{\\beta_{j}, \\epsilon_{i}}} \\; M  \\;\\;$  subject    to \n",
    "#### <center> $\\sum^{p}_{j=1}\\beta_{j}^2 = 1, \\quad y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + ... + \\beta_{p}x_{ip}) \\geq M(1-\\epsilon_{i})$ </center>\n",
    "$\\epsilon_{i} \\geq0, \\;\\; \\sum^{n}_{i=1}\\epsilon_{i} \\leq C, \\;\\; where \\;\\;C > 0$ is a __tuning parameter__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img5](img5.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\epsilon_{1},..,\\epsilon_{n}$ 은 __slack variables__라 부른다.\n",
    "\n",
    "- if $\\; \\epsilon_{i} = 0 \\;$, the $i$th observation is on the __correct__ side in the __margin__. (2nd obs, 9th obs)\n",
    "- if $\\; \\epsilon_{i} > 0 \\;$, the $i$th observation is on the __wrong__ side in the __margin__. (1st obs, 8th obs)\n",
    "- if $\\; \\epsilon_{i} > 1 \\;$, the $i$th observation is on the __wrong__ side out of the __margin__. (11th obs, 12th obs)\n",
    "<br>\n",
    "\n",
    "#### $C$는 tunning parameter로써, regularization 역할을 한다.\n",
    "- if $\\;C \\;$ is small, then narrow margin (high variance, low bias)\n",
    "- if $\\;C \\;$ is large, then narrow margin (low variance, high bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img6](img6.JPG) <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "만약 두 범주가 __linear__로 분류가 가능하다면, support vector classfier는 좋은 모델이다. 하지만, 현실적으로 __non-linear__인 경우가 충분히 존재할 수 있다. 아래의 그림을 보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img7](img7.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림과 같은 데이터에서는 support vector classfier로 아무리 $C$값을 tuning한다 하더라도, 좋은 결과를 기대하기 어렵다. <br>\n",
    "그렇다면 이와 같은 문제를 해결하기 위해 어떻게 모델을 만들어야할까? (-> __Feature Expansion__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Expansion\n",
    "비선형 데이터를 고차원으로 mapping 해주는 것이다. 이렇게 만들어진 새로운 차원에서는 hyperplane으로 선형분류가 가능해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img8](img8.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림과 같이 2차원 데이터들을 3차원으로 mapping 해주게 되면 두 범주를 선형으로써 분류가 가능해진다. 하지만 데이터를 고차원으로 mapping 해주게 될 경우, 계산 비용이 엄청나게 증가한다.(__curse of dimensionality__ : 차원의 저주) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick\n",
    "앞 내용에서는 생략되었지만, support vector classifier를 만들기 위해선, 개별 데이터간의 __inner product__가 필요하다. 즉, $_{n}C_{2}$만큼의 inner product를 수행하는데, 차원을 늘린다면 계산 비용이 엄청나게 늘어난다. <br>\n",
    "하지만, 위 그림에서 보았듯이, 초평면으로 데이터를 분류하기 위해서는 고차원으로 mapping은 필수적이다. 그렇다면 어떻게 이를 해결할 수 있을까? <br>\n",
    "고차원으로 mapping후 연산 결과가 수식적으로 저차원에서 연산결과와 동일하게 되도록, 모델을 만들어 주면 된다. <br>\n",
    "kernel trick에 대한 자세한 내용은 아래 링크 참조 (회귀분석 모델로써 간단한 예제로 linear kernel trick을 설명한다) <br>\n",
    "[kernel trick](https://www.youtube.com/watch?v=szQaKuVhYFU&t=1429s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대표적으로 다음과 같은 3가지 커널이 있다.\n",
    "- Standard Linear Kernel\n",
    "<center>$K(x_{i},x_{i'})\\;=\\; \\sum^{p}_{j=1}x_{ij}x_{i'j}$ </center>\n",
    "- Polynomial kernel of degree d > 1\n",
    "<center>$K(x_{i},x_{i'})\\;=\\; (1+\\sum^{p}_{j=1}x_{ij}x_{i'j})^{d} \\;\\;$ where $d$ is tunning parameter </center>  \n",
    "- Radial kernel (Gaussian kernel)\n",
    "<center>$K(x_{i},x_{i'})\\;=\\; exp\\{-\\gamma \\sum^{p}_{j=1}(x_{ij}-x_{i'j})^2\\} \\;\\;$ where $\\gamma$ is tunning parameter</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img9](img9.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Left : SVM with a polynimail kernel of $d$(degree) 3\n",
    "- Right : SVM with a radial kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
