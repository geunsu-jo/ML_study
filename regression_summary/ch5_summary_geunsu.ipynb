{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀분석(Regression Analysis)\n",
    "회귀분석은 여러 __독립변수__와 한개의 __종속변수__간의 상관관계를 모델링하는 기법이다. <br>\n",
    "회귀분석의 핵심은 데이터기반 학습을 통해 최적의 회귀계수를 찾는 것이다. <br>\n",
    "### 회귀분석의 특징\n",
    "1. 회귀분석은 독립변수의 개수에 따라 __단일회귀__와 __다중회귀__로 나눈다. \n",
    "2. 회귀계수의 선형성에 따라 __선형회귀__와 __비선형회귀__로 나뉜다. (독립변수가 비선형인지는 중요하지 않다.)\n",
    "3. 종속변수의 유형에 따라 분류(classificaiton) 문제도 적용가능하다.\n",
    "4. 정규화를 위해 규제 적용이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 회귀(Linear Regression)\n",
    "#### $y_{i} = w_{0} + w_{1}*x_{i,1} + w_{1}*x_{i,1} + ..... + w_{p}*x_{i,p}$ <br>\n",
    "위와같이 회귀계수가 선형인 모델을 __선형회귀__라 부른다. <br>\n",
    "특별한 경우로, p=1일때, __단순선형회귀__라고 부른다. <br>\n",
    "__오차__(관측값과 실제값의 차이)를 최소화하도록 하는 회귀계수를 찾는다. <br>\n",
    "오차 : $r_{i} = y_{i} - (w_{0} + w_{1}*x_{i})$\n",
    "![image.png](img1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차를 통해 회귀계수를 구하는 대표적인 2가지 방법이 있다.\n",
    "1. $\\sum \\lvert (y_{i} - (w_{0} + w_{1}*x_{i}))\\rvert$를 최소로하는 회귀계수 $w_{0}, w_{1}$\n",
    "2. $\\sum(y_{i} - (w_{0} + w_{1}*x_{i}))^2$를 최소로하는 회귀계수 $w_{0}, w_{1}$\n",
    "\n",
    "실제 회귀분석에서는 미분이 가능한 2의 식을 주로 사용한다. <br>\n",
    "(1의 식은 오차를 lapacian distribution을 가정한 모델의 MLE를 Maximize하는 과정과 동치) <br>\n",
    "(2의 식은 오차를 normal distribution을 가정한 모델 MLE를 Maximize하는 과정과 동치)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사 하강법\n",
    "점진적으로 반복적인 계산을 통해 미분계수가 0인 해를 찾는 방법. <br>\n",
    "회귀분석에서 RSS를 최소화하는 $w_{0},w_{1}$를 찾기위해 경사 하강법을 사용한다.\n",
    "\n",
    "\n",
    "![image1.png](img2.PNG)\n",
    "위의 그림에서 비용함수는 \n",
    "$RSS(w_{0},w_{1}) = \\sum(y_{i} - (w_{0} + w_{1}*x_{i}))^2$ <br>\n",
    "대부분 일반적인 회귀분석(선형이 아닌 경우)엔, 회귀계수를 구하기 위해 경사하강법을 사용하지만, <br>\n",
    "특별한 경우로, 선형회귀에서는 명시적(explict)인 해를 구할 수 있어, 회귀계수를 구하기 위한 반복적인 계산은 필요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다항회귀(Polynomial regression)\n",
    "#### $y_{i} = w_{0} + w_{1}*x_{i}^2 + w_{1}*x_{i}^3 +.... +  w_{p}*x_{i}^p$ \n",
    "![image2.png](img3.PNG)\n",
    "그림과 같이 데이 2차식 혹은 3차식 등 다항식으로 표현하는 회귀분석을 다항회귀분석이라고 한다. <br>\n",
    "주의할 점은 회귀계수는 선형이므로 다항회귀 역시 __선형회귀의 일부__이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다항회귀의 과적합(Overfitting)문제\n",
    "다항회귀에서 $p$값(degree, 차수, 여기서는 회귀계수의 개수와 동일)를 정하는 것 또한 중요한 문제이다. <br>\n",
    "다항회귀 역시 선형회귀분석의 일부이므로 RSS를 최소로하는 회귀계수와 회귀계수의 수($p$)를 구한다. <br>\n",
    "하지만 __회귀계수의 수가 증가할수록 RSS는 반드시 감소__한다.(잔차를 없애는 복잡한 모형을 만들 수 있으므로)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image3.png](img4.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검은색은 실제 회귀식을 나타내고(Only god knows), 노란색, 하늘색, 초록색은 각각 $p$ 값의 증가를 나타낸다.\n",
    "- 오른쪽 그림의 빨간 선은 test set의 MSE 변화, 회색 선은 training set의 MSE 변화이다.\n",
    "- training set은 $p$가 증가할수록 MSE는 감소하지만, test set은 특정시점부터 MSE가 증가함을 알 수 있다.\n",
    "- 따라서 $p$는 __bias-variance trade off__ 관계를 갖고 적절한 $p$를 선택하는 것 또한 중요한 문제다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 규제 선형 회귀(Regularization Linear Regression)\n",
    "선형회귀에 규제(Regularization)를 적용한 회귀이다. <br>\n",
    "대표적으로 Ridge Regression과 LASSO가 있다. 또한, 이들을 결합한 Elastic Net\n",
    "### 규제 선형 회귀의 필요성\n",
    "RSS를 최소화하기 위해선, 회귀계수의 수를 무한으로 늘려야한다. 하지만 앞에서 보았듯이, 회귀계수의 수가 증가하면 과적합(Overfitting)문제가 발생한다. 어떻게 이 문제를 해결할 수 있을까? 회귀계수의 수를 적당히 선택하도록(가능한 적은 수의 회귀계수가 선택되도록) 규제(혹은 Penalty)를 가하면된다. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "#### $Minimize$  :  $RSS + \\alpha * \\sum w_{j}^2$ \n",
    "Penalty term : $\\alpha * \\sum w_{j}^2$, ($L_{2}$ Regularization) <br>\n",
    "Hyperparameter : $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO\n",
    "\n",
    "#### $Minimize$  :  $RSS + \\alpha * \\sum \\lvert w_{j}\\rvert$ \n",
    "Penalty term : $\\alpha * \\sum \\lvert w_{j}\\rvert$, ($L_{1}$ Regularization) <br>\n",
    "Hyperparameter : $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ridge와 LASSO 모두 RSS에 추가적으로 penalty term이 더해졌다. <br>\n",
    "2. 회귀계수의 수($w_{j}$의 수)가 많아지면 Penalty는 커짐을 알 수 있다.(즉, 이러한 규제는 회귀계수를 작게 선택하도록한다)\n",
    "3. $\\alpha$가 0이면 선형회귀와 동일한 회귀계수를 갖는다.\n",
    "4. $\\alpha$가 커지면, 규제(Regularizaiton)은 증가한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge보다 LASSO가 선호되는 이유\n",
    "![image4.png](img5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\beta$는 회귀계수로 $w$와 동일하고, 왼쪽은 __LASSO__, 오른쪽은 __Ridge__\n",
    "- RIdge는 불필요한 회귀계수를 0에 근사시키지만 정확한 0의 값으로 만들지 않는다.\n",
    "- __LASSO는 불필요한 회귀계수를 정확히 0의 값으로 만들어준다. (즉, 변수선택 문제에 적용이 가능하다.)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net \n",
    "\n",
    "Ridge와 LASSO를 결합한 규제 선형 회귀로, $\\alpha$가 추가되어 총 2개의 Hyperparameter\n",
    "\n",
    "#### $Minimize$  :  $RSS + \\lambda * \\alpha_{1}\\sum w_{j}^2 + (1-\\lambda) * \\alpha_{2}\\sum \\lvert w_{j} \\rvert$  \n",
    "Penalty term : $\\alpha * \\sum w_{j}^2$, ($L_{2}$ Regularization) <br>\n",
    "Hyperparameter : $\\alpha_{1}$, $\\alpha_{2}$, $\\lambda$ $    where \\lambda ∈ [0, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀(Logistic Regression)\n",
    "로지스틱 회귀는 종속변수 Y가 범주형 변수(Categorical Variable)일때 사용하는 일반선형회귀(Generalized Linear Regression)의 한 부분이다. <br>\n",
    "로지스틱 회귀는 말 그대로 회귀이지만, 실제로 분류 문제에 많이 적용된다. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y가 범주형일때, 단순선형회귀(Simple Linear Regression)의 구조적 결함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image5.png](img6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 그림에서 노란색은 data를, 하늘색은 회귀식을 나타낸다.\n",
    "- 왼쪽 그림과 같이 단순선형회귀(Simple Linear Regression)을 적용하게 되면 회귀식이 data를 잘 설명하지 못하는 구조적인 결함을 가진다.\n",
    "- 따라서 오른쪽 그림과 같이 로지스틱회귀를 적용하여, 이러한 구조적인 결함을 해결 할 수있다.\n",
    "- 이러한 구조적 결함을 해결하도록 하는 함수를 __시그모이드 함수(Sigmoid Function)__라 한다.\n",
    "\n",
    "### $Sigmoid : $ $ y = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀의 분류문제 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image6.png](img7.PNG)\n",
    "- 그림과 같이 로지스틱 회귀에 적절한 역치값(threshold value)를 설정하여 분류 문제에 적용이 가능하다.\n",
    "- threshold value에 따라 결과가 달라질 수 있으므로 이 또한 Hyperparameter이다.\n",
    "- 하지만 통상적으로 0.5를 사용하는것이 일반적이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
